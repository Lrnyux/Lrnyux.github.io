
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<title>Ego+X: An Egocentric Vision System for Global 3D Human Pose Estimation and Social Interaction Characterization</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

	<!-- Meta tags for Zotero grab citation -->
	<meta name="citation_title" content="Ego+X: An Egocentric Vision System for Global 3D Human Pose Estimation and Social Interaction Characterization">
	<meta name="citation_author" content="Liu, Yuxuan">
	<meta name="citation_author" content="Yang, Jianxin">
	<meta name="citation_author" content="Gu, Xiao">
	<meta name="citation_author" content="Guo, Yao">
	<meta name="citation_author" content="Yang, Guang-Zhong">
	<meta name="citation_publication_date" content="2022">
	<meta name="citation_conference_title" content="IROS">
	<meta name="keywords" content="egocentric, 3D pose estimation, social behavior">
	<meta name="citation_pdf_url" content="[pdf]">

	<meta name="robots" content="index,follow">
	<meta name="description" content="We propose Ego+X, an egocentric vision based system for 3D canonical pose estimation and human-centric social interaction characterization. Our system is composed of two head-mounted egocentric cameras, where one is faced downwards and the other looks outwards.">
	<link rel="author" href="https://sjtulyx.github.io/"/>

	<!-- Fonts and stuff -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" href="css/project.css" media="screen" />
	<link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />
	<script src="js/google-code-prettify/prettify.js"></script>
	<style>
	.container{
    width: 768px;
    overflow:hidden;
    display:block;
    height: 432px;
	margin: 0 auto;
	autoplay:"autoplay";
	loop:"loop"
	}
	#vid{
		margin-left: -5px;
		autoplay:"autoplay";
	loop:"loop"
	}
	</style>
</head>

<body>
	<div id="content">
		<div id="content-inner">
			<div class="section logos"  style="text-align:center">
				<a href="https://imr.sjtu.edu.cn/" target="_blank"><img src="picture/imr_logo.jpg" height="55"></a>
				<a href="https://bme.sjtu.edu.cn/" target="_blank"><img src="picture/bme_logo.jpg" height="55"></a>             
			</div>

			<div class="section head">
			
				<h1>Ego+X: An Egocentric Vision System for Global 3D Human Pose Estimation and Social Interaction Characterization</h1>

				<div class="authors">
					<a href="https://people.mpi-inf.mpg.de/~jianwang/" target="_blank">Yuxuan Liu</a><sup>1</sup>&#160;&#160;
					<a href="https://lingjie0206.github.io/" target="_blank">Jianxin Yang</a><sup>1</sup>&#160;&#160;
					<a href="https://sites.google.com/view/xuweipeng" target="_blank">Xiao Gu</a><sup>2</sup>&#160;&#160; 
					<a href="https://people.mpi-inf.mpg.de/~ksarkar/" target="_blank">Yao Guo</a><sup>1</sup>&#160;&#160; 
					<a href="https://dluvizon.github.io/" target="_blank">Guang-Zhong Yang</a><sup>1</sup>&#160;&#160; 
				</div>

				<div class="affiliations">
					<sup>1</sup><a href="http://www.mpi-inf.mpg.de/" target="_blank">Max Planck Institute for Informatics</a>, <a href="https://saarland-informatics-campus.de/" target="_blank">Saarland Informatics Campus</a> &#160;&#160;
                    <sup>2</sup><a href="https://research.fb.com/category/augmented-reality-virtual-reality/" target="_blank">facebook Reality Labs</a>
				</div>

				 <div class="venue"> CVPR</div> 
				
			</div>
			<div class="section teaser">
				<div class="container">
                <video id="vid" width="768" height="432" controls><source src="data/camera_ready.mp4" type="video/mp4"></video>
				</div>
			</div>

			<div class="section abstract">
				<h2>Abstract</h2>
				<p>
				Egocentric 3D human pose estimation with a single fisheye camera has drawn a significant amount of attention recently. However, existing methods struggle with pose estimation from in-the-wild images, because they can only be trained on synthetic data due to the unavailability of large-scale in-the-wild egocentric datasets. Furthermore, these methods easily fail when the body parts are occluded by or interacting with the surrounding scene. 
				To address the shortage of in-the-wild data, we collect a large-scale in-the-wild egocentric dataset called Egocentric Poses in the Wild (EgoPW). This dataset is captured by a head-mounted fisheye camera and an auxiliary external camera, which provides an additional observation of the human body from a third-person perspective during training.
				We present a new egocentric pose estimation method, which can be trained on the new dataset with weak external supervision. 
				Specifically, we first generate pseudo labels for the EgoPW dataset with a spatio-temporal optimization method by incorporating the external-view supervision.
				The pseudo labels are then used to train an egocentric pose estimation network. To facilitate the network training, we propose a novel learning strategy to supervise the egocentric features with the high-quality features extracted by a pretrained external-view pose estimation model. 
				The experiments show that our method predicts accurate 3D poses from a single in-the-wild egocentric image and outperforms the state-of-the-art methods both quantitatively and qualitatively. 
				</p>
			</div>

			<div class="section downloads">
				<h2>Downloads</h2>  
				<center>
				<ul>
					<li class="grid">
						<div class = "griditem">
							<a href="data/egopw.pdf" target="_blank" class="imageLink"><img src = "images/pdf.jpg"></a><br/>
							Paper<br/>
						</div>
					</li>
					
					<li class="grid">
						<div class = "griditem">
							<a href="data/supp_mat.pdf" target="_blank" class="imageLink"><img src = "images/pdf.jpg"></a><br/>
							Suppl. Mat.<br/>
						</div>
					</li>
 
					<li class="grid"> 
						<div class = "griditem"> 
						<a href="data/camera_ready.mp4" target="_blank" class="imageLink"><img src = "images/mp4.jpg"></a><br />
						Video<br /> 
						<br /> 
						</div>
					</li> 
					
					<li class="grid">
						<div class = "griditem">
							<a href="data/LICENSE.txt" target="_blank" class="imageLink"><img src = "images/txt.jpg"></a><br/>
							Data License<br/>
						</div>
					</li>
					
					<li class="grid"> 
						<div class = "griditem"> 
						<a href="https://nextcloud.mpi-klsb.mpg.de/index.php/s/5yikSw26sXy3wrR" target="_blank" class="imageLink"><img src = "images/zip.jpg"></a><br/>
						Dataset<br /> 
						<br /> 
						</div>
					</li> 
					
				</ul>
				</center>
			</div> 
			
			<br />
			
			
 
			<div class="section list">
				<h2>Citation</h2>
				<p><a href="data/bibtex.bib" target="_blank">BibTeX, 1 KB</a></p>
				<div class="section bibtex">
				<pre>
@article{wang2022estimating,
  title={Estimating Egocentric 3D Human Pose in the Wild with External Weak Supervision},
  author={Wang, Jian and Liu, Lingjie and Xu, Weipeng and Sarkar, Kripasindhu and Luvizon, Diogo and Theobalt, Christian},
  journal={CVPR},
  year={2022}
}
				</div>
			</div> 


			<div class="section acknowledgments">
				<h2>Acknowledgments</h2>
				<p>
				Jian Wang, Kripasindhu Sarkar and Christian Theobalt have been supported by the ERC Consolidator Grant 4DReply (770784) and Lingjie Liu has been supported by Lise Meitner Postdoctoral Fellowship. We also thank Gereon Fox for his help with the narration on the supplementary video.
				</p>
			</div>

			<div class="section contact">
				<h2>Contact</h2>
				For questions, clarifications, please get in touch with:<br />
				Jian Wang <a href='mailt&#111;&#58;jianwang&#64;mp&#105;-inf&#46;mp&#103;&#46;&#100;e'>jianwang&#64;mp&#105;-inf&#46;mp&#103;&#46;&#100;e</a>  <br />
                Lingjie Liu <a href='mailt&#111;&#58;lliu&#64;&#109;p&#105;&#45;inf&#46;%6D&#37;&#55;0%67&#46;&#100;e'>lliu&#64;mp&#105;-inf&#46;mp&#103;&#46;&#100;e</a> 
			</div>

			<div class="section">
				<hr class="smooth">
				This page is <a href="http://www.zotero.org" target="_blank">Zotero</a> translator friendly. Page last updated 
				<script type="text/javascript">
					var m = "This page was last updated: " + document.lastModified;
					var p = m.length-9;
					document.writeln("<left>");
					document.write(m.substring(p, 0) + ".");
					document.writeln("</left>");
				</script>
				<a href="https://imprint.mpi-klsb.mpg.de/inf/vcai.mpi-inf.mpg.de/">Imprint</a>. <a href="https://data-protection.mpi-klsb.mpg.de/inf/vcai.mpi-inf.mpg.de?lang=en">Data Protection</a>.
			</div>
		</div>
	</div>
</body>
</html>
