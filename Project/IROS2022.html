
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<title>Ego+X: An Egocentric Vision System for Global 3D Human Pose Estimation and Social Interaction Characterization</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

	<!-- Meta tags for Zotero grab citation -->
	<meta name="citation_title" content="Ego+X: An Egocentric Vision System for Global 3D Human Pose Estimation and Social Interaction Characterization">
	<meta name="citation_author" content="Liu, Yuxuan">
	<meta name="citation_author" content="Yang, Jianxin">
	<meta name="citation_author" content="Gu, Xiao">
	<meta name="citation_author" content="Guo, Yao">
	<meta name="citation_author" content="Yang, Guang-Zhong">
	<meta name="citation_publication_date" content="2022">
	<meta name="citation_conference_title" content="IROS">
	<meta name="keywords" content="egocentric, 3D pose estimation, social behavior">
	<meta name="citation_pdf_url" content="[pdf]">

	<meta name="robots" content="index,follow">
	<meta name="description" content="We propose Ego+X, an egocentric vision based system for 3D canonical pose estimation and human-centric social interaction characterization. Our system is composed of two head-mounted egocentric cameras, where one is faced downwards and the other looks outwards.">
	<link rel="author" href="https://sjtulyx.github.io/"/>

	<!-- Fonts and stuff -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" href="css/project.css" media="screen" />
	<link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />
	<script src="js/google-code-prettify/prettify.js"></script>
	<style>
	.container{
    width: 768px;
    overflow:hidden;
    display:block;
    height: 432px;
	margin: 0 auto;
	autoplay:"autoplay";
	loop:"loop"
	}
	#vid{
		margin-left: -5px;
		autoplay:"autoplay";
	loop:"loop"
	}
	</style>
</head>

<body>
	<div id="content">
		<div id="content-inner">
			<div class="section logos"  style="text-align:center">
				<a href="https://imr.sjtu.edu.cn/" target="_blank"><img src="picture/imr_logo.jpg" height="55"></a>
				<a href="https://bme.sjtu.edu.cn/" target="_blank"><img src="picture/bme_logo.jpg" height="55"></a>             
			</div>

			<div class="section head">
			
				<h1>Ego+X: An Egocentric Vision System for Global 3D Human Pose Estimation and Social Interaction Characterization</h1>

				<div class="authors">
					Yuxuan Liu<sup>1</sup>&#160;&#160;
					Jianxin Yang<sup>1</sup>&#160;&#160;
					Xiao Gu<sup>2</sup>&#160;&#160; 
					Yao Guo<sup>1</sup>&#160;&#160; 
					Guang-Zhong Yang<sup>1</sup>&#160;&#160; 
				</div>

				<div class="affiliations">
					<sup>1</sup>Institute of Medical Robotics, Shanghai Jiao Tong University &#160;&#160;
                    <sup>2</sup>Hamlyn Center, Imperial College London, UK
				</div>

				 <div class="venue"> IROS</div> 
				
			</div>
			<div class="section teaser">
				<div class="container">
                <video id="vid" width="768" height="432" controls><source src="[video]" type="video/mp4"></video>
				</div>
			</div>

			<div class="section abstract">
				<h2>Abstract</h2>
				<p>
				Egocentric vision is an emerging topic, which has demonstrated great potential in assistive healthcare scenarios, ranging from human-centric behavior analysis to personal social assistance. Within this field, due to the heterogeneity of visual perception from first-person views, egocentric pose estimation is one of the most significant prerequisites for enabling various downstream applications. However, existing methods for egocentric pose estimation mainly focus on predicting the pose represented in the camera coordinates from a single image, which ignores the latent cues in the temporal domain and results in less accuracy. In this paper, we propose Ego+X, an egocentric vision based system for 3D canonical pose estimation and human-centric social interaction characterization. Our system is composed of two head-mounted egocentric cameras, where one is faced downwards and the other looks outwards.
                                By leveraging the global context provided by visual SLAM, we first propose \textit{Ego-Glo} for spatial-accurate and temporal-consistent egocentric 3D pose estimation in the canonical coordinate system. With the help of an egocentric camera looking outwards, we then propose \textit{Ego-Soc} by extending Ego-Glo to various social interaction tasks, e.g., object detection and human-human interaction. Quantitative and qualitative experiments have been conducted to demonstrate the effectiveness of our proposed Ego+X. 
				</p>
			</div>

			<div class="section downloads">
				<h2>Downloads</h2>  
				<center>
				<ul>
					<li class="grid">
						<div class = "griditem">
							<a href="data/egopw.pdf" target="_blank" class="imageLink"><img src = "images/pdf.jpg"></a><br/>
							Paper<br/>
						</div>
					</li>
					
				</ul>
				</center>
			</div> 
			
			<br />
			
			
 
			<div class="section list">
				<h2>Citation</h2>
				<div class="section bibtex">
				<pre>
@article{liu2022Ego+X,
  title={Ego+X: An Egocentric Vision System for Global 3D Human Pose Estimation and Social Interaction Characterization},
  author={Liu, Yuxuan and Yang, Jianxin and Gu, Xiao and Guo, Yao and Yang, Guang-Zhong},
  journal={IROS},
  year={2022}
}
				</div>
			</div> 


			<div class="section acknowledgments">
				<h2>Acknowledgments</h2>
				<p>
				This work was supported by the Science and Technology Commission of Shanghai Municipality under Grant 20DZ2220400
				</p>
			</div>


			<div class="section">
				<hr class="smooth">
				This page is <a href="http://www.zotero.org" target="_blank">Zotero</a> translator friendly. Page last updated 
				<script type="text/javascript">
					var m = "This page was last updated: " + document.lastModified;
					var p = m.length-9;
					document.writeln("<left>");
					document.write(m.substring(p, 0) + ".");
					document.writeln("</left>");
				</script>
			</div>
		</div>
	</div>
</body>
</html>
